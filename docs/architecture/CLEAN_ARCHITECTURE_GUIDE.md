# Clean Architecture Excel Processor

A scalable, modular application for uploading, parsing, and processing Excel/CSV files using Clean Architecture principles.

## ðŸ—ï¸ Architecture Overview

This application follows **Clean Architecture** (also known as Ports & Adapters, Hexagonal Architecture), which separates concerns into distinct layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Interface Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Streamlit   â”‚  â”‚   FastAPI    â”‚  â”‚     CLI      â”‚  â”‚
â”‚  â”‚     UI       â”‚  â”‚     API      â”‚  â”‚  Interface   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  Application Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Use Cases:                                       â”‚  â”‚
â”‚  â”‚  â€¢ FileUploadUseCase                              â”‚  â”‚
â”‚  â”‚  â€¢ DataTransformationUseCase                      â”‚  â”‚
â”‚  â”‚  â€¢ DataExportUseCase                              â”‚  â”‚
â”‚  â”‚  â€¢ JobManagementUseCase                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Domain Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Entities:                                        â”‚  â”‚
â”‚  â”‚  â€¢ FileMetadata                                   â”‚  â”‚
â”‚  â”‚  â€¢ DataFrameWrapper                               â”‚  â”‚
â”‚  â”‚  â€¢ ValidationResult                               â”‚  â”‚
â”‚  â”‚  â€¢ ColumnSchema                                   â”‚  â”‚
â”‚  â”‚                                                   â”‚  â”‚
â”‚  â”‚  Interfaces (Protocols):                          â”‚  â”‚
â”‚  â”‚  â€¢ FileParserProtocol                             â”‚  â”‚
â”‚  â”‚  â€¢ DataTransformerProtocol                        â”‚  â”‚
â”‚  â”‚  â€¢ ValidatorProtocol                              â”‚  â”‚
â”‚  â”‚  â€¢ RepositoryProtocol                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 Infrastructure Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Pandas    â”‚  â”‚  In-Memory   â”‚  â”‚  File-based  â”‚   â”‚
â”‚  â”‚  Parser    â”‚  â”‚  Repository  â”‚  â”‚  Repository  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Null      â”‚  â”‚  Column      â”‚  â”‚   Type       â”‚   â”‚
â”‚  â”‚  Cleaner   â”‚  â”‚  Renamer     â”‚  â”‚  Converter   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“¦ Layer Responsibilities

### 1. Domain Layer (Innermost)
- **Contains**: Business entities, value objects, domain logic
- **Dependencies**: None (pure Python)
- **Key Components**:
  - `FileMetadata`: File information (name, type, size, sheets)
  - `DataFrameWrapper`: Data container with metadata
  - `ValidationResult`: Validation outcome
  - `ColumnSchema`: Column definition and constraints
  - Protocols: Define interfaces for outer layers

### 2. Application Layer
- **Contains**: Use cases, business logic orchestration
- **Dependencies**: Only Domain layer
- **Key Components**:
  - `FileUploadUseCase`: Upload and validate files
  - `DataTransformationUseCase`: Apply transformations
  - `DataExportUseCase`: Export to various formats
  - `JobManagementUseCase`: Manage processing jobs

### 3. Infrastructure Layer
- **Contains**: External implementations (parsers, storage)
- **Dependencies**: Domain layer (implements protocols)
- **Key Components**:
  - `PandasExcelParser`: Parse Excel/CSV using pandas
  - `FileValidator`: Validate file format and integrity
  - `InMemoryRepository`: Temporary storage
  - Transformers: `NullValueCleaner`, `ColumnRenamer`, `TypeConverter`
  - Exporters: `ExcelExporter`, `CSVExporter`, `JSONExporter`

### 4. Interface Layer (Outermost)
- **Contains**: UI, API, CLI implementations
- **Dependencies**: Application layer
- **Key Components**:
  - `api.py`: FastAPI REST API
  - `streamlit_app.py`: Streamlit web interface

## ðŸ”„ Data Flow

### File Upload Flow

```
1. User uploads file (UI/API)
   â†“
2. Interface Layer receives file
   â†“
3. Application Layer (FileUploadUseCase)
   â”œâ”€ Validates file (FileValidator)
   â”œâ”€ Parses file (PandasExcelParser)
   â”œâ”€ Validates schema (if provided)
   â””â”€ Saves to repository
   â†“
4. Returns job ID and validation results
```

### Transformation Flow

```
1. User requests transformation with job ID
   â†“
2. Application Layer (DataTransformationUseCase)
   â”œâ”€ Loads data from repository
   â”œâ”€ Applies transformations in sequence
   â”‚   â”œâ”€ NullValueCleaner
   â”‚   â”œâ”€ ColumnRenamer
   â”‚   â””â”€ TypeConverter
   â””â”€ Saves transformed data
   â†“
3. Returns transformation summary
```

### Export Flow

```
1. User requests export (format: excel/csv/json)
   â†“
2. Application Layer (DataExportUseCase)
   â”œâ”€ Loads data from repository
   â”œâ”€ Selects appropriate exporter
   â””â”€ Generates file
   â†“
3. Interface Layer sends file to user
```

## ðŸš€ Quick Start

### Option 1: FastAPI + Streamlit (Full Stack)

```bash
# Terminal 1: Start Backend
cd src_clean_architecture
python -m interface.api

# Terminal 2: Start Streamlit UI
streamlit run src_clean_architecture/interface/streamlit_app.py
```

### Option 2: Programmatic Usage

```python
from src_clean_architecture.domain import ColumnSchema
from src_clean_architecture.application import FileUploadUseCase
from src_clean_architecture.infrastructure import (
    PandasExcelParser,
    FileValidator,
    InMemoryRepository,
    NullValueCleaner
)

# Initialize dependencies
repository = InMemoryRepository()
parser = PandasExcelParser()
validator = FileValidator()

# Create use case
upload_use_case = FileUploadUseCase(validator, parser, repository)

# Execute
job = upload_use_case.execute(
    file_path="data.xlsx",
    filename="data.xlsx",
    expected_schema=[
        ColumnSchema(name="id", dtype="int", required=True),
        ColumnSchema(name="name", dtype="str", required=True),
        ColumnSchema(name="amount", dtype="float", required=True)
    ]
)

print(f"Job ID: {job.job_id}")
print(f"Validation: {job.validation_results[0].message}")
```

## ðŸ“‹ Features

### File Validation
- âœ… Extension checking (.xlsx, .xls, .csv)
- âœ… File size limits
- âœ… Corruption detection
- âœ… Schema validation
- âœ… Type checking

### Data Transformations
- ðŸ§¹ **NullValueCleaner**: Remove or fill null values
- ðŸ“ **ColumnRenamer**: Rename columns
- ðŸ”„ **TypeConverter**: Convert data types
- ðŸ“Š **CustomTransformers**: Easy to add new ones

### Export Formats
- ðŸ“— Excel (.xlsx)
- ðŸ“„ CSV
- ðŸ“‹ JSON

## ðŸ§ª Testing

```python
# Test domain logic without external dependencies
from src_clean_architecture.domain import FileMetadata, FileType

metadata = FileMetadata(
    filename="test.xlsx",
    file_type=FileType.XLSX,
    size_bytes=1024,
    row_count=100,
    column_count=5
)

assert metadata.size_mb == 0.001
```

## ðŸ›ï¸ Architecture Benefits

### 1. **Testability**
- Domain logic tested without databases, UI, or external services
- Mock implementations for testing

### 2. **Framework Independence**
- Domain doesn't know about pandas, FastAPI, or Streamlit
- Easy to swap implementations

### 3. **Maintainability**
- Changes in outer layers don't affect inner layers
- Clear separation of concerns

### 4. **Scalability**
- Easy to add new transformers
- Easy to add new export formats
- Easy to add new UI implementations

## ðŸ“ Example: Adding a Custom Transformer

```python
# 1. Implement the protocol in Infrastructure layer
from src_clean_architecture.domain import DataFrameWrapper

class CustomTransformer:
    def get_name(self) -> str:
        return "custom_transform"
    
    def get_description(self) -> str:
        return "Apply custom business logic"
    
    def transform(self, data: DataFrameWrapper) -> DataFrameWrapper:
        df = data.data
        # Your transformation logic here
        df['new_column'] = df['existing'] * 2
        data.data = df
        return data

# 2. Register in application
from src_clean_architecture.application import DataTransformationUseCase

transformers = [
    NullValueCleaner(),
    CustomTransformer()  # Add here
]

transform_use_case = DataTransformationUseCase(repository, transformers)
```

## ðŸ“Š Project Structure

```
src_clean_architecture/
â”œâ”€â”€ domain/                    # Business logic (no dependencies)
â”‚   â”œâ”€â”€ __init__.py           # Entities, protocols, enums
â”‚   â”œâ”€â”€ entities.py           # Additional entities
â”‚   â””â”€â”€ exceptions.py         # Domain exceptions
â”‚
â”œâ”€â”€ application/              # Use cases
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ use_cases.py          # Business logic orchestration
â”‚
â”œâ”€â”€ infrastructure/           # External implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ parsers.py            # Pandas, validators, transformers
â”‚
â””â”€â”€ interface/                # UI/API layer
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ api.py                # FastAPI REST API
    â””â”€â”€ streamlit_app.py      # Streamlit web interface
```

## ðŸ”§ Configuration

Create a `.env` file:

```
# API Settings
API_HOST=0.0.0.0
API_PORT=8000

# File Upload
MAX_FILE_SIZE_MB=100
UPLOAD_DIR=./uploads

# Storage
STORAGE_TYPE=memory  # memory, file, database
```

## ðŸŽ¯ Next Steps

1. **Add Database Storage**: Implement `DatabaseRepository`
2. **Add Authentication**: JWT tokens in Interface layer
3. **Add More Transformers**: Aggregations, filters, joins
4. **Add Async Processing**: Celery for background jobs
5. **Add Caching**: Redis for frequently accessed data

## ðŸ“š References

- [Clean Architecture by Robert C. Martin](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)
- [Hexagonal Architecture](https://alistair.cockburn.us/hexagonal-architecture/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Streamlit Documentation](https://docs.streamlit.io/)

---

**Version**: 2.0.0 | **License**: MIT
